{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f4meR_PRU8P1",
        "hwJcxqPZWPtm",
        "pvIS39JzP7qB",
        "qdR9ADuaXBGC",
        "bBB4kfNrPzl5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByFEg7XuIs0h",
        "colab_type": "text"
      },
      "source": [
        "# **Hyperparameters tuning for Convolutional Neural Network based on Global Optimization** \n",
        "\n",
        "---\n",
        "\n",
        "![Global Optimization](http://www.globaloptimization.org/wp-content/uploads/2017/09/GKLS-e1507106893485-300x216.png) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4meR_PRU8P1",
        "colab_type": "text"
      },
      "source": [
        "# **Requirements**\n",
        "\n",
        "Here we download the libraries for our project (bayesian-optimization, optunity) and we define a class for using the pytorch dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJcxqPZWPtm",
        "colab_type": "text"
      },
      "source": [
        "## ***Pip install libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riMpaW8CVYMJ",
        "colab_type": "code",
        "outputId": "6ef16bd1-f544-4d06-a8e0-c96884793018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=bd258ef537cccd0a1e4b85bd6e538cb4944210b628414cee851c436ae4bd7618\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4O6bF4VUrVL",
        "colab_type": "code",
        "outputId": "54d72509-8120-4f61-dfce-9e4b2b1f1614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "pip install optunity\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optunity\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/4d/d49876a49e105b56755eb5ba06a4848ee8010f7ff9e0f11a13aefed12063/Optunity-1.1.1.tar.gz (4.6MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6MB 1.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: optunity\n",
            "  Building wheel for optunity (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optunity: filename=Optunity-1.1.1-cp36-none-any.whl size=72024 sha256=d04ef9f4e68d9d8ec9806e411e93966613f51590e7c0922eed5ba62a5b5198b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/e2/80/d3794524ae0042e147e035e132ec5fac09b8794b4acd94f046\n",
            "Successfully built optunity\n",
            "Installing collected packages: optunity\n",
            "Successfully installed optunity-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvIS39JzP7qB",
        "colab_type": "text"
      },
      "source": [
        "## ***Database Menagement***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd7IyZoEP81u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class ChunkSampler(sampler.Sampler):\n",
        "    \"\"\"Samples elements sequentially from some offset.\n",
        "    Arguments:\n",
        "        num_samples: # of desired datapoints\n",
        "        start: offset where we should start selecting from\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_samples, start=0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "def getDataset(validation=False,dataset_name='mnist'):\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    nw = 4      # number of workers threads\n",
        "    bs = 64     # batch size\n",
        "\n",
        "    if dataset_name == 'mnist':\n",
        "        train_size = 60000\n",
        "    if dataset_name == 'cifar10':\n",
        "        train_size = 50000\n",
        "\n",
        "    if validation:\n",
        "        if dataset_name == 'mnist':\n",
        "            train_size = 50000\n",
        "        if dataset_name == 'cifar10':\n",
        "            train_size = 40000\n",
        "        validation_size = 10000\n",
        "\n",
        "\n",
        "    if dataset_name == 'mnist':\n",
        "        train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    if dataset_name == 'cifar10':\n",
        "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "        #FIXME here found a methods for remove the printed line from console log 'Files already downloaded and verified'\n",
        "        # print('\\r')  # back to previous line\n",
        "        # comando per cancellare o rimuvere quella scritta\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bs, shuffle=False, num_workers=nw, sampler=ChunkSampler(train_size, 0))\n",
        "\n",
        "    if dataset_name == 'mnist':\n",
        "        test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    if dataset_name == 'cifar10':\n",
        "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs, shuffle=False, num_workers=nw)\n",
        "\n",
        "    if validation:\n",
        "        validation_loader = torch.utils.data.DataLoader(train_set, batch_size=bs, shuffle=False, num_workers=nw, sampler=ChunkSampler(validation_size, train_size))\n",
        "        return train_loader, validation_loader, test_loader\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdR9ADuaXBGC",
        "colab_type": "text"
      },
      "source": [
        "# ***`Parameters `***   \n",
        "\n",
        "1.   Number of evaluations\n",
        "2.   Number of init points for bayesian\n",
        "3.   Max epoches for eval on neural network\n",
        "4.   Name of output file\n",
        "5.   Hyperparameters domains\n",
        "6.   Dataset name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLV1V1guYM0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# param for experiment\n",
        "output_file = 'result.csv'\n",
        "evaluations = 25\n",
        "init_points = 5\n",
        "max_epochs = 50\n",
        "# here is possible to select MNIST of CIFAR10 dataset\n",
        "dataset_name= 'cifar10' # mnist\n",
        "# gpu id for colab or gpu on pc\n",
        "gpu = 0\n",
        "\n",
        "# hyperparameters domains\n",
        "hyperparameters = {\"learning_rate\": (0.0001, 0.1), \"weight_decay\": (0, 0.001)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBB4kfNrPzl5",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg-H4TyYPppA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, learning_rate, weight_decay, epochs, gpu, dataset_name):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        if dataset_name == 'cifar10':\n",
        "            self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "        if dataset_name == 'cifar10':\n",
        "            self.fc = nn.Linear(500, 10)\n",
        "\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        self.max_epochs = epochs\n",
        "\n",
        "        # indicate if the network module is created through training\n",
        "        self.fitted = False\n",
        "\n",
        "        #set criterion\n",
        "        self.criterion = F.nll_loss\n",
        "\n",
        "        # selection of device to use\n",
        "        self.device = torch.device(\"cuda:\" + str(gpu) if torch.cuda.is_available() and gpu is not None else \"cpu\")\n",
        "        self.gpu = gpu\n",
        "        if self.device == \"cpu\":\n",
        "            self.gpu = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "  #FIXME eventualmente ricontrollare\n",
        "    def init_net(self, m):\n",
        "        #reset all parameters for Conv2d layer\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.reset_parameters()\n",
        "            # m.weight.data.fill_(0.01)\n",
        "            # m.bias.data.fill_(0.01)\n",
        "        #reset all parameters for Linear layer\n",
        "        if isinstance(m, nn.Linear):\n",
        "            m.weight.data.fill_(0.01)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.apply(self.init_net)\n",
        "\n",
        "    def fit(self, train_loader):\n",
        "        # same initial point for all the network\n",
        "        self.reset_parameters()\n",
        "        self.train()\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        for epochs in range(self.max_epochs):\n",
        "            # debug line\n",
        "            # print('epochs:'+epochs.__str__())\n",
        "            for data in train_loader:\n",
        "                x,y=data\n",
        "                if self.gpu is not None:\n",
        "                    x, y = x.to(self.device), y.to(self.device)\n",
        "                # if torch.cuda.is_available():\n",
        "                #     x, y = x.cuda(), y.cuda()\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self(x)\n",
        "                train_loss = self.criterion(output, y)\n",
        "                train_loss.backward()\n",
        "                self.optimizer.step()\n",
        "        self.fitted = True\n",
        "        return train_loss\n",
        "\n",
        "    def validation(self, validation_loader):\n",
        "        if not self.fitted:\n",
        "            exit(1)\n",
        "        else:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            loss = 0.0\n",
        "            num_batches = 0\n",
        "            with torch.no_grad():\n",
        "                for data in validation_loader:\n",
        "                    # get some test images\n",
        "                    x, y = data\n",
        "                    if self.gpu is not None:\n",
        "                        x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "                    # images classes prediction\n",
        "                    outputs = self(x)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                    # loss update\n",
        "                    loss += self.criterion(outputs, y).item()\n",
        "                    num_batches += 1\n",
        "\n",
        "                    # update numbers of total and correct predictions\n",
        "                    total += y.size(0)\n",
        "                    correct += (predicted == y).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "            loss /= num_batches\n",
        "            return loss, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp81mBy9yfgh",
        "colab_type": "text"
      },
      "source": [
        "## Print the network with *`torchsummary`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rawztmMwrN4",
        "colab_type": "code",
        "outputId": "080f24d1-ef64-4e3e-d454-1ee14967cf25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net(learning_rate=0.0001, weight_decay=0.01, epochs=max_epochs, gpu=0, dataset_name='cifar10').to(device)\n",
        "\n",
        "if dataset_name == 'mnist':\n",
        "  summary(model,(1, 28, 28))\n",
        "if dataset_name == 'cifar10':\n",
        "  summary(model,(3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 28, 28]             760\n",
            "         MaxPool2d-2           [-1, 10, 14, 14]               0\n",
            "            Conv2d-3           [-1, 20, 10, 10]           5,020\n",
            "         MaxPool2d-4             [-1, 20, 5, 5]               0\n",
            "            Linear-5                   [-1, 10]           5,010\n",
            "================================================================\n",
            "Total params: 10,790\n",
            "Trainable params: 10,790\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.09\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.15\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0aFENtQGXU",
        "colab_type": "text"
      },
      "source": [
        "## ***Evaluation***\n",
        "\n",
        "Here we have the metod for evaluate the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv2tbzCcQH9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import optunity\n",
        "import csv\n",
        "\n",
        "def evaluate(learning_rate, weight_decay):\n",
        "    device = torch.device('cuda:' + gpu.__str__() if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net(learning_rate, weight_decay, max_epochs, gpu, dataset_name=dataset_name).to(device)\n",
        "    train_loader, validation_loader, test_loader = getDataset(validation=True, dataset_name=dataset_name)\n",
        "    training_losses = model.fit(train_loader)\n",
        "    validation_losses, validation_accuracy = model.validation(test_loader)\n",
        "    best_val_loss = validation_losses\n",
        "\n",
        "    # print(\"Accuracy Validation: \" + str(validation_accuracy))\n",
        "    # print('--------')\n",
        "    # print('Learning rate, weight decay')\n",
        "    # print(learning_rate,weight_decay)\n",
        "\n",
        "    # Save results in csv\n",
        "    with open(output_file, 'a') as file:\n",
        "        my_csv_row = 'iter,' + best_val_loss.__str__() + ',' + learning_rate.__str__() + ',' + weight_decay.__str__() + ',' + validation_accuracy.__str__() + '\\n'\n",
        "        file.write(my_csv_row)\n",
        "\n",
        "    return -best_val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJNxlx1WQSlR",
        "colab_type": "text"
      },
      "source": [
        "# **Test**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT45H_VvQTzs",
        "colab_type": "code",
        "outputId": "eea1d2b5-5035-4007-c8b7-74e5d2f0b23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "# from optimization import evaluate_BAY\n",
        "from bayes_opt import BayesianOptimization\n",
        "import optunity\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def bayesian():\n",
        "    bayesian = BayesianOptimization(f=evaluate, pbounds=hyperparameters)\n",
        "    bayesian.maximize(init_points=init_points, n_iter=evaluations - init_points)\n",
        "    return '\\nResults with Bayesian optimizer: ' + str(bayesian.max) + '\\n'\n",
        "\n",
        "def quasi_random():\n",
        "    quasi_random=optunity.maximize(f=evaluate, num_evals=evaluations, solver_name='sobol', learning_rate=[0.0001, 0.1], weight_decay=[0, 0.001])\n",
        "    return '\\nResult with quasiRandom optimizer: ' + str(quasi_random) + '\\n'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    now = datetime.now()\n",
        "    with open(output_file, 'a', newline='') as file:\n",
        "        file.write('\\nBayesian ' + now.isoformat() + '\\n')\n",
        "        file.write('\\nIter, Loss, Learning Rate, Weight Decay, Accuracy Validation\\n')\n",
        "\n",
        "    result_bay = bayesian()\n",
        "    print(result_bay)\n",
        "\n",
        "    with open(output_file, 'a') as file:\n",
        "        file.write(result_bay + '\\n')\n",
        "\n",
        "    print('------------')\n",
        "\n",
        "    now = datetime.now()\n",
        "    with open(output_file, 'a') as file:\n",
        "        file.write('QuasiRandom ' + now.isoformat() + '\\n')\n",
        "        file.write('\\nIter, Loss, Learning Rate, Weight Decay, Accuracy Validation\\n')\n",
        "\n",
        "\n",
        "    result_qr = quasi_random()\n",
        "    print(result_qr)\n",
        "\n",
        "    with open(output_file, 'a') as file:\n",
        "        file.write(result_qr+'\\n')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | learni... | weight... |\n",
            "-------------------------------------------------\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 26984529.29it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}